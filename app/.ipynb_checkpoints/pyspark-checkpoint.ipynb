{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lookup candidate entities and classes\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import argparse\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "current_path = os.getcwd()\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--input_dir',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'data'),\n",
    "    help='Directory of input/output')\n",
    "parser.add_argument(\n",
    "    '--file_type',\n",
    "    type=str,\n",
    "    default='csv',\n",
    "    help='File type')\n",
    "parser.add_argument(\n",
    "    '--lookup_results_rank',\n",
    "    type=int,\n",
    "    default=5,\n",
    "    help='File type')\n",
    "\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "# if not os.path.exists(FLAGS.input_dir):\n",
    "#     os.mkdir(FLAGS.input_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the csv files from the input directory\n",
    "def get_data_files(data_folder):\n",
    "    \"\"\"\n",
    "    A function used to get all the csv files from the input directory\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    data_folder : str\n",
    "        the folder within  the working directory where the data is located\n",
    "    \"\"\"\n",
    "\n",
    "    files = [] # a list of all filenames, including file extensions, that contain data\n",
    "    csv_files = [] # same list as above but without the file extension\n",
    "\n",
    "    # Get the list of files\n",
    "    files = [f for f in os.listdir(FLAGS.input_dir+data_folder) if os.path.isfile(os.path.join(FLAGS.input_dir+data_folder, f))]\n",
    "    csv_files = [f.replace(\".csv\",\"\") for f in os.listdir(FLAGS.input_dir+data_folder) if os.path.isfile(os.path.join(FLAGS.input_dir+data_folder, f))]\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "def get_target_cta_columns(target_config_file, data_folder, csv_files, filter_col = True):\n",
    "    \"\"\"\n",
    "    A function used to get which columns from the csv files need to be considered for the CTA. This is a subset of the file columns ignoring anything that is not an entity\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    target_config_file : str\n",
    "        the file that contains the target column indices for each file\n",
    "    csv_files : list\n",
    "        the list of csv files that have the tabular data\n",
    "    filter_col : boolean\n",
    "        a flag to indicate whether we should narrow down the reading of the columns to only those targeted for the CTA task\n",
    "    \"\"\"\n",
    "   \n",
    "    target_col_file = os.path.join(FLAGS.input_dir+data_folder, target_config_file)\n",
    "    df_target_col = pd.read_csv(target_col_file,header=None, names=['filename','column_index'])\n",
    "    \n",
    "    # filter to only those files that are included in the csv_files\n",
    "    df_target_col = df_target_col.loc[df_target_col['filename'].isin(csv_files)]\n",
    "    \n",
    "    # collapse all rows pertaining to the same file into one key value pair. The key is the filename and the value is the list with the column indices that should be considered\n",
    "    # dict_target = {'CTRL_DBP_GEO_european_countries_capital_populated_cities': [0, 1, 2]}\n",
    "    dict_target = dict()\n",
    "    \n",
    "    for index,row in df_target_col.iterrows():\n",
    "        \n",
    "        # is this is the first row with this file create the key\n",
    "        if row['filename'] not in dict_target:\n",
    "            dict_target[row['filename']]= []\n",
    "            \n",
    "        # append the new target column to the target column list for that file\n",
    "        if filter_col:\n",
    "            dict_target[row['filename']].append(int(row['column_index']))\n",
    "    \n",
    "    return dict_target\n",
    "\n",
    "def get_ground_truth(file, folder, csv_files):\n",
    "    \"\"\"\n",
    "    A function used to get the ground truths as provided in the setup\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    file : str\n",
    "        the file that contains the ground truth for the class of each column in each file\n",
    "    folder : str\n",
    "        the folder that contains the ground truth file\n",
    "    csv_files : list\n",
    "        the list of csv files that have the tabular data\n",
    "    \"\"\"\n",
    "    \n",
    "    dbo_prefix = 'http://dbpedia.org/ontology/'\n",
    "   \n",
    "    filepath = os.path.join(FLAGS.input_dir+folder, file)\n",
    "    df_ground_truth = pd.read_csv(filepath,header=None, names=['filename','column_index', 'class'])\n",
    "    \n",
    "    # filter to only those files that are included in the csv_files\n",
    "    df_ground_truth = df_ground_truth.loc[df_ground_truth['filename'].isin(csv_files)]\n",
    "    \n",
    "    # collapse all rows pertaining to the same file into one key value pair. The key is the filename and the value is the list with the column indices that should be considered\n",
    "    # dict_target = {'CTRL_DBP_GEO_european_countries_capital_populated_cities': [0, 1, 2]}\n",
    "    dict_gt = dict()\n",
    "    \n",
    "    for index,row in df_ground_truth.iterrows():\n",
    "        \n",
    "        # is this is the first row with this file create the key\n",
    "        if row['filename'] not in dict_gt:\n",
    "            dict_gt[row['filename']]= dict()\n",
    "            \n",
    "        # append the new target column to the target column list for that file\n",
    "        dict_gt[row['filename']][int(row['column_index'])] = row['class'].split(dbo_prefix)[1]\n",
    "    \n",
    "    return dict_gt\n",
    "\n",
    "def read_data(data_folder, dict_target_col, has_header_row = False):\n",
    "    \"\"\"\n",
    "    A function used to read the data from the csvs in the data_folder only considering the columns that are in the dict_target_col\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    folder : str\n",
    "        the folder that contains the csvs with the tabular data\n",
    "    dict_target_col : dictionary\n",
    "        a dictionary with csv filenames as the key and an array of relevant column indices as a value\n",
    "    has_header_row : boolean\n",
    "        a flag to indicate whether the first row in the csv files needs to be skipped as it is a header\n",
    "    \"\"\"\n",
    "    data = list()\n",
    "\n",
    "    for file in dict_target_col:\n",
    "        element = dict()\n",
    "        element['filename'] = file\n",
    "        df_data = pd.DataFrame()\n",
    "        df_title = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "        filename = file + '.' + FLAGS.file_type\n",
    "        tab_data_file = os.path.join(FLAGS.input_dir + data_folder, filename)\n",
    "\n",
    "        # read the file data in a dataframe. Also read the column titles if we need to use them\n",
    "        if len(dict_target_col[file])>0:\n",
    "            if has_header_row:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None, skiprows=[0], usecols=dict_target_col[file])\n",
    "                df_title = pd.read_csv(tab_data_file,header=None, usecols=dict_target_col[file], nrows = 1)\n",
    "            else:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None, usecols=dict_target_col[file])\n",
    "        else:\n",
    "            if has_header_row:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None, skiprows=[0])\n",
    "                df_title = pd.read_csv(tab_data_file,header=None, nrows = 1)\n",
    "            else:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None)\n",
    "\n",
    "        # add the column headers to the data dictionary\n",
    "        try:\n",
    "            element['column_titles'] = list(df_title.iloc[0,:])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        file_element = dict()\n",
    "        for column in df_data.columns:\n",
    "            file_element[column] = list(set(df_data[column]))\n",
    "        element['data'] = file_element\n",
    "\n",
    "        element['dataframe'] = df_data    \n",
    "        data.append(element)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Setup\n",
    "\n",
    "As part of this initial step we will need to load the data we are going to process as well as the targets we are trying to meet. The data is located in the data folder as follows\n",
    "- round_1:\n",
    "    - gt: the expected outcome (ground truth)\n",
    "    - tables: the tabular data\n",
    "    - targets: the columns / cells we need to consider for the CTA/CEA\n",
    "----\n",
    "Step 1: Get a list of all the csv files in the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10579449_0_1681126353774891032',\n",
       " '11833461_1_3811022039809817402',\n",
       " '13719111_1_5719401842463579519',\n",
       " '14067031_0_559833072073397908',\n",
       " '1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5',\n",
       " '1438042986423_95_20150728002306-00329-ip-10-236-191-2_805336391_10',\n",
       " '1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2',\n",
       " '1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2',\n",
       " '14380604_4_3329235705746762392',\n",
       " '16767252_0_2409448375013995751',\n",
       " '20135078_0_7570343137119682530',\n",
       " '21245481_0_8730460088443117515',\n",
       " '21362676_0_6854186738074119688',\n",
       " '22864497_0_8632623712684511496',\n",
       " '24036779_0_5608105867560183058',\n",
       " '25404227_0_2240631045609013057',\n",
       " '26310680_0_5150772059999313798',\n",
       " '28086084_0_3127660530989916727',\n",
       " '29414811_12_251152470253168163',\n",
       " '29414811_13_8724394428539174350',\n",
       " '29414811_2_4773219892816395776',\n",
       " '29414811_6_8221428333921653560',\n",
       " '33401079_0_9127583903019856402',\n",
       " '34041816_1_4749054164534706977',\n",
       " '35188621_0_6058553107571275232',\n",
       " '36102169_0_7739454799295072814',\n",
       " '37856682_0_6818907050314633217',\n",
       " '38428277_0_1311643810102462607',\n",
       " '39107734_2_2329160387535788734',\n",
       " '39173938_0_7916056990138658530',\n",
       " '39650055_5_7135804139753401681',\n",
       " '39759273_0_1427898308030295194',\n",
       " '40534006_0_4617468856744635526',\n",
       " '41336118_0_4331895026409635103',\n",
       " '41480166_0_6681239260286218499',\n",
       " '43237185_1_3636357855502246981',\n",
       " '45073662_0_3179937335063201739',\n",
       " '46671561_0_6122315295162029872',\n",
       " '50245608_0_871275842592178099',\n",
       " '50270082_0_444360818941411589',\n",
       " '52299421_0_4473286348258170200',\n",
       " '53822652_0_5767892317858575530',\n",
       " '53989675_0_8697482470743954630',\n",
       " '54719588_0_8417197176086756912',\n",
       " '58891288_0_1117541047012405958',\n",
       " '60319454_0_3938426910282115527',\n",
       " '63450419_0_8012592961815711786',\n",
       " '66009064_0_9148652238372261251',\n",
       " '69537082_0_7789694313271016902',\n",
       " '69881946_0_1105130426898457358',\n",
       " '71840765_0_6664391841933033844',\n",
       " '75367212_2_2745466355267233390',\n",
       " '77694908_0_6083291340991074532',\n",
       " '80588006_0_6965325215443683359',\n",
       " '84548468_0_5955155464119382182',\n",
       " '84575189_0_6365692015941409487',\n",
       " '8468806_0_4382447409703007384',\n",
       " '88523363_0_8180214313099580515',\n",
       " '90196673_0_5458330029110291950',\n",
       " '91959037_0_7907661684242014480',\n",
       " '9475172_1_1023126399856690702',\n",
       " '9567241_0_5666388268510912770',\n",
       " '9834884_0_3871985887467090123',\n",
       " '99070098_0_2074872741302696997']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of csv files with tabular data\n",
    "csv_files = get_data_files('\\\\round_1\\\\tables')\n",
    "# csv_files = csv_files[:1]\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Get the columns we need to consider for the CTA task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'58891288_0_1117541047012405958': [1, 3],\n",
       " '8468806_0_4382447409703007384': [1, 2],\n",
       " '50245608_0_871275842592178099': [0, 3, 4],\n",
       " '14067031_0_559833072073397908': [1, 7, 5, 0],\n",
       " '39759273_0_1427898308030295194': [1, 3],\n",
       " '14380604_4_3329235705746762392': [1, 2],\n",
       " '20135078_0_7570343137119682530': [3, 1],\n",
       " '29414811_6_8221428333921653560': [1, 4, 2],\n",
       " '34041816_1_4749054164534706977': [2, 1],\n",
       " '29414811_2_4773219892816395776': [1, 4, 2],\n",
       " '99070098_0_2074872741302696997': [1],\n",
       " '35188621_0_6058553107571275232': [3, 1],\n",
       " '43237185_1_3636357855502246981': [3, 0, 2],\n",
       " '46671561_0_6122315295162029872': [0, 1],\n",
       " '11833461_1_3811022039809817402': [0, 1],\n",
       " '21245481_0_8730460088443117515': [1, 0],\n",
       " '38428277_0_1311643810102462607': [3, 1],\n",
       " '13719111_1_5719401842463579519': [0],\n",
       " '77694908_0_6083291340991074532': [1, 3],\n",
       " '88523363_0_8180214313099580515': [0, 2],\n",
       " '1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2': [0],\n",
       " '25404227_0_2240631045609013057': [3, 1],\n",
       " '9475172_1_1023126399856690702': [1],\n",
       " '60319454_0_3938426910282115527': [0],\n",
       " '52299421_0_4473286348258170200': [1],\n",
       " '37856682_0_6818907050314633217': [0],\n",
       " '29414811_13_8724394428539174350': [2, 4, 1],\n",
       " '33401079_0_9127583903019856402': [0],\n",
       " '26310680_0_5150772059999313798': [1, 0],\n",
       " '10579449_0_1681126353774891032': [1],\n",
       " '84548468_0_5955155464119382182': [3, 1],\n",
       " '63450419_0_8012592961815711786': [1],\n",
       " '39107734_2_2329160387535788734': [1, 2],\n",
       " '29414811_12_251152470253168163': [4, 2, 1],\n",
       " '91959037_0_7907661684242014480': [1],\n",
       " '75367212_2_2745466355267233390': [1, 2, 0],\n",
       " '84575189_0_6365692015941409487': [2],\n",
       " '40534006_0_4617468856744635526': [1],\n",
       " '1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5': [0],\n",
       " '22864497_0_8632623712684511496': [0, 1],\n",
       " '1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2': [0],\n",
       " '28086084_0_3127660530989916727': [0],\n",
       " '9834884_0_3871985887467090123': [1, 2],\n",
       " '69881946_0_1105130426898457358': [1, 3],\n",
       " '39650055_5_7135804139753401681': [1],\n",
       " '16767252_0_2409448375013995751': [3, 1],\n",
       " '71840765_0_6664391841933033844': [0],\n",
       " '36102169_0_7739454799295072814': [1, 2],\n",
       " '45073662_0_3179937335063201739': [1],\n",
       " '53822652_0_5767892317858575530': [3, 1],\n",
       " '39173938_0_7916056990138658530': [1, 3],\n",
       " '21362676_0_6854186738074119688': [3, 1],\n",
       " '80588006_0_6965325215443683359': [1],\n",
       " '90196673_0_5458330029110291950': [0],\n",
       " '53989675_0_8697482470743954630': [0],\n",
       " '66009064_0_9148652238372261251': [0, 2],\n",
       " '50270082_0_444360818941411589': [1],\n",
       " '24036779_0_5608105867560183058': [0, 5],\n",
       " '41336118_0_4331895026409635103': [0],\n",
       " '54719588_0_8417197176086756912': [3, 5, 4],\n",
       " '41480166_0_6681239260286218499': [0],\n",
       " '9567241_0_5666388268510912770': [1],\n",
       " '69537082_0_7789694313271016902': [1]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the columns we need to consider for the CTA task\n",
    "dict_target_col = get_target_cta_columns('CTA_Round1_Targets.csv', '\\\\round_1\\\\targets', csv_files,True)\n",
    "dict_target_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Get the ground truth for all columns in the set of csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'58891288_0_1117541047012405958': {1: 'Film', 3: 'Person'},\n",
       " '8468806_0_4382447409703007384': {1: 'Lake', 2: 'Country'},\n",
       " '50245608_0_871275842592178099': {0: 'Film', 3: 'Person', 4: 'Writer'},\n",
       " '14067031_0_559833072073397908': {1: 'Language',\n",
       "  7: 'Currency',\n",
       "  5: 'City',\n",
       "  0: 'Country'},\n",
       " '39759273_0_1427898308030295194': {1: 'Film', 3: 'Person'},\n",
       " '14380604_4_3329235705746762392': {1: 'Company', 2: 'PopulatedPlace'},\n",
       " '20135078_0_7570343137119682530': {3: 'Person', 1: 'Film'},\n",
       " '29414811_6_8221428333921653560': {1: 'VideoGame', 4: 'Company', 2: 'Genre'},\n",
       " '34041816_1_4749054164534706977': {2: 'City', 1: 'Airport'},\n",
       " '29414811_2_4773219892816395776': {1: 'VideoGame', 4: 'Company', 2: 'Genre'},\n",
       " '99070098_0_2074872741302696997': {1: 'Mountain'},\n",
       " '35188621_0_6058553107571275232': {3: 'Person', 1: 'Film'},\n",
       " '43237185_1_3636357855502246981': {3: 'Scientist',\n",
       "  0: 'Scientist',\n",
       "  2: 'EducationalInstitution'},\n",
       " '46671561_0_6122315295162029872': {0: 'VideoGame', 1: 'Genre'},\n",
       " '11833461_1_3811022039809817402': {0: 'VideoGame', 1: 'Company'},\n",
       " '21245481_0_8730460088443117515': {1: 'Lake', 0: 'AdministrativeRegion'},\n",
       " '38428277_0_1311643810102462607': {3: 'Person', 1: 'Film'},\n",
       " '13719111_1_5719401842463579519': {0: 'Bird'},\n",
       " '77694908_0_6083291340991074532': {1: 'Film', 3: 'Person'},\n",
       " '88523363_0_8180214313099580515': {0: 'Plant', 2: 'Eukaryote'},\n",
       " '1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2': {0: 'Mountain'},\n",
       " '25404227_0_2240631045609013057': {3: 'Person', 1: 'Film'},\n",
       " '9475172_1_1023126399856690702': {1: 'TelevisionShow'},\n",
       " '60319454_0_3938426910282115527': {0: 'Animal'},\n",
       " '52299421_0_4473286348258170200': {1: 'Country'},\n",
       " '37856682_0_6818907050314633217': {0: 'AdministrativeRegion'},\n",
       " '29414811_13_8724394428539174350': {2: 'Genre', 4: 'Company', 1: 'VideoGame'},\n",
       " '33401079_0_9127583903019856402': {0: 'Person'},\n",
       " '26310680_0_5150772059999313798': {1: 'Currency', 0: 'Country'},\n",
       " '10579449_0_1681126353774891032': {1: 'Newspaper'},\n",
       " '84548468_0_5955155464119382182': {3: 'Person', 1: 'Film'},\n",
       " '63450419_0_8012592961815711786': {1: 'Plant'},\n",
       " '39107734_2_2329160387535788734': {1: 'Airport', 2: 'City'},\n",
       " '29414811_12_251152470253168163': {4: 'Company', 2: 'Genre', 1: 'VideoGame'},\n",
       " '91959037_0_7907661684242014480': {1: 'VideoGame'},\n",
       " '75367212_2_2745466355267233390': {1: 'Company',\n",
       "  2: 'Company',\n",
       "  0: 'VideoGame'},\n",
       " '84575189_0_6365692015941409487': {2: 'AcademicJournal'},\n",
       " '40534006_0_4617468856744635526': {1: 'Wrestler'},\n",
       " '1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5': {0: 'PoliticalParty'},\n",
       " '22864497_0_8632623712684511496': {0: 'VideoGame', 1: 'Company'},\n",
       " '1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2': {0: 'Cricketer'},\n",
       " '28086084_0_3127660530989916727': {0: 'Saint'},\n",
       " '9834884_0_3871985887467090123': {1: 'Museum', 2: 'Country'},\n",
       " '69881946_0_1105130426898457358': {1: 'Film', 3: 'Person'},\n",
       " '39650055_5_7135804139753401681': {1: 'Company'},\n",
       " '16767252_0_2409448375013995751': {3: 'Person', 1: 'Film'},\n",
       " '71840765_0_6664391841933033844': {0: 'Lake'},\n",
       " '36102169_0_7739454799295072814': {1: 'Company', 2: 'VideoGame'},\n",
       " '45073662_0_3179937335063201739': {1: 'BaseballPlayer'},\n",
       " '53822652_0_5767892317858575530': {3: 'Person', 1: 'Film'},\n",
       " '39173938_0_7916056990138658530': {1: 'Film', 3: 'Person'},\n",
       " '21362676_0_6854186738074119688': {3: 'Person', 1: 'Film'},\n",
       " '80588006_0_6965325215443683359': {1: 'Newspaper'},\n",
       " '90196673_0_5458330029110291950': {0: 'Animal'},\n",
       " '53989675_0_8697482470743954630': {0: 'Lake'},\n",
       " '66009064_0_9148652238372261251': {0: 'BaseballPlayer', 2: 'SportsTeam'},\n",
       " '50270082_0_444360818941411589': {1: 'BaseballPlayer'},\n",
       " '24036779_0_5608105867560183058': {0: 'Country', 5: 'GovernmentType'},\n",
       " '41336118_0_4331895026409635103': {0: 'Company'},\n",
       " '54719588_0_8417197176086756912': {3: 'Film', 5: 'Person', 4: 'Country'},\n",
       " '41480166_0_6681239260286218499': {0: 'Saint'},\n",
       " '9567241_0_5666388268510912770': {1: 'Newspaper'},\n",
       " '69537082_0_7789694313271016902': {1: 'Country'}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = get_ground_truth('CTA_Round1_gt.csv', '\\\\round_1\\\\gt', csv_files)\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "The next step is to load the data from the csv files. We load the data as an array of dictionaries.\n",
    "Each dictionary will have the following structure:<br>\n",
    "{<br>\n",
    "<blockquote>\n",
    "<strong>'filename':</strong> '1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5',<br>\n",
    "<strong>'column_titles'</strong>: ['Party'],<br>\n",
    "<strong>'data'</strong>: <br>\n",
    "    {<br>\n",
    "    <blockquote>\n",
    "        <strong>0:</strong> ['PC', 'Lib-Dem','SNP','UKIP','Labour','BNP','Conservative','Green']<br>\n",
    "    </blockquote>\n",
    "        },<br>\n",
    "\n",
    "<strong>'dataframe':</strong>               0\n",
    " 0  Conservative\n",
    " 1        Labour\n",
    " 2       Lib-Dem\n",
    " 3           SNP\n",
    " 4            PC\n",
    " 5         Green\n",
    " 6           BNP\n",
    " 7          UKIP<br>\n",
    "</blockquote>    \n",
    " }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data('\\\\round_1\\\\tables', dict_target_col,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "\n",
    "def retrieve_dbpedia_classes (query_string, max_hits = 5):\n",
    "    web_api = 'http://lookup.dbpedia.org/api/search/KeywordSearch?MaxHits=%s&QueryString=%s'\n",
    "    dbo_prefix = 'http://dbpedia.org/ontology/'\n",
    "    dbp_prefix = 'http://dbpedia.org/resource/'\n",
    "    entity_classes = dict()\n",
    "    try:\n",
    "        lookup_url = web_api % (max_hits, query_string)\n",
    "#         print(lookup_url)\n",
    "        lookup_res = requests.get(lookup_url)\n",
    "        root = ET.fromstring(lookup_res.content)\n",
    "        i=0\n",
    "        for child in root:\n",
    "            i+=1\n",
    "#             print(\"\\n\")\n",
    "            entity = child[1].text.split(dbp_prefix)[1]\n",
    "#             print(entity)\n",
    "            classes = list()\n",
    "            for cc in child[3]:\n",
    "                cls_URI = cc[1].text\n",
    "#                 print(cls_URI)\n",
    "                if dbo_prefix in cls_URI:\n",
    "                    classes.append((cls_URI.split(dbo_prefix)[1]))\n",
    "            if len(classes)>0:\n",
    "                entity_classes[entity] = dict()\n",
    "                entity_classes[entity]['rank'] = i\n",
    "                entity_classes[entity]['candidate_classes'] = classes\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "    return entity_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup cell_values\n",
    "\n",
    "With the data loaded in the *data* dictionary the next step is to lookup the cell values in the DBpedia endpoint and get the canidate classes and entities.\n",
    "Each cell value is only looked up once, however we still keep track of any column it might have appeared in as well as all candidate entities and classes it may have matched to.\n",
    "\n",
    "For this level of analysis we are flexible to store the 5 top lookup results for each cell value (default value for FLAGS.lookup_results_rank).\n",
    "We will then assess the number of classifiers we need to train later and perhaps filter out any candidate classes that only appeared in lower ranks.\n",
    "\n",
    "The outcome of the lookup is stored in the *cell_values* dictionary as follows:\n",
    "\n",
    "{<strong>\"Madagascar\":</strong><br>\n",
    "{\n",
    "<blockquote><strong>\"location\":</strong> [(\"14067031_0_559833072073397908\",0)]\n",
    "            , <br><strong>\"candidate_entities\":</strong><br> \n",
    "                        {\n",
    "                            <blockquote><strong>\"Madagascar\":</strong> <br>{<blockquote><strong>\"rank\":</strong> 1,<br> <strong>\"candidate_classes\":</strong> [\"Place\", \"Country\", \"PopulatedPlace\", \"Location\"]</blockquote>}, <br>\n",
    "                            <strong>\"Antananarivo\":</strong> <br> {<blockquote><strong>\"rank\":</strong> 3,<br> <strong>\"candidate_classes\":</strong> [\"Settlement\", \"Place\", \"PopulatedPlace\", \"Location\"]</blockquote>}, <br>\n",
    "                            <strong>\"List_of_Madagascar_(franchise)_characters\":</strong> <br> {<blockquote><strong>\"rank\":</strong> 4,<br> <strong>\"candidate_classes\":</strong> [\"FictionalCharacter\", \"Agent\"]</blockquote>},<br>\n",
    "                            <strong>\"Madagascar_national_football_team\"</strong> <br> {<blockquote><strong>\"rank\":</strong> 5,<br> <strong>\"candidate_classes\":</strong> [\"Organisation\", \"SoccerClub\", \"Agent\", \"SportsClub\"]</blockquote>}<br>\n",
    "</blockquote>}<br> \n",
    "</blockquote>},<br>\n",
    "               \n",
    " <strong>\"South Africa\":</strong> {...},<br>\n",
    "  ...<br>\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00 --> 58891288_0_1117541047012405958 :  Jean-Luc Godard\n"
     ]
    }
   ],
   "source": [
    "cell_values = dict()\n",
    "i = 0\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "size = 0\n",
    "for file_i in range(len(data)):\n",
    "    for col in data[file_i]['data']:\n",
    "        for line_j in range(len(data[file_i]['data'][col])):\n",
    "            size+=1\n",
    "            \n",
    "start_time = time.time()\n",
    "\n",
    "# from tqdm import tqdm\n",
    "for file_i in range(len(data)):\n",
    "#     print(data[file_i])\n",
    "    filename = data[file_i]['filename']\n",
    "    for col in data[file_i]['data']:\n",
    "        column_index = col\n",
    "#         print(col)\n",
    "#         print(data[file_i]['data'][col])\n",
    "        for line_j in range(len(data[file_i]['data'][col])):\n",
    "            i+=1\n",
    "            cell_value = data[file_i]['data'][col][line_j]\n",
    "            clear_output(wait=True)\n",
    "            print('{0:.2f}'.format(100*i/size,2),'-->',filename, \": \",cell_value)\n",
    "            if cell_value in cell_values.keys():\n",
    "                cell_values[cell_value]['location'].append((filename,column_index))\n",
    "            else:\n",
    "                cell_values[cell_value] = dict()\n",
    "                cell_values[cell_value]['location'] = [(filename,column_index)]\n",
    "                try:\n",
    "                    cell_values[cell_value]['candidate_entities'] = retrieve_dbpedia_classes(cell_value.replace(\"[\",'').replace(\"]\",''),FLAGS.lookup_results_rank)\n",
    "                except:\n",
    "                    cell_values[cell_value]['candidate_entities'] = retrieve_dbpedia_classes(cell_value,FLAGS.lookup_results_rank)\n",
    "                \n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"{int(end_time - start_time)//60} min and {int((end_time - start_time)%60)} seconds Elapsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{int(end_time - start_time)//60} min and {int((end_time - start_time)%60)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_dbpedia_classes('Mozambique',FLAGS.lookup_results_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('cell_values.json', 'w') as fp:\n",
    "    json.dump(cell_values, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cell_values.json') as json_file:\n",
    "    loaded_cell_values = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AdministrativeRegion',\n",
       " 'Place',\n",
       " 'PopulatedPlace',\n",
       " 'Location',\n",
       " 'Region',\n",
       " 'Settlement',\n",
       " 'Town',\n",
       " 'HistoricPlace',\n",
       " 'Criminal',\n",
       " 'Person',\n",
       " 'SportsTeam',\n",
       " 'AmericanFootballTeam',\n",
       " 'Building',\n",
       " 'ArchitecturalStructure',\n",
       " 'Village',\n",
       " 'River',\n",
       " 'Stream',\n",
       " 'Lake',\n",
       " 'Organisation',\n",
       " 'School',\n",
       " 'EducationalInstitution',\n",
       " 'Agent',\n",
       " 'SoccerLeague',\n",
       " 'SportsLeague',\n",
       " 'SoccerClub',\n",
       " 'SportsClub',\n",
       " 'Politician']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_classes = list([])\n",
    "candidate_classes_rank = list([])\n",
    "for key in entity_classes:\n",
    "    for candicate_class in entity_classes[key]:\n",
    "        cc,rank = candicate_class\n",
    "        if cc not in candidate_classes:\n",
    "            candidate_classes.append(cc)\n",
    "            candidate_classes_rank.append((cc,rank))\n",
    "\n",
    "candidate_classes_rank = sorted(candidate_classes_rank, key=lambda x: x[1])\n",
    "[t[0] for t in candidate_classes_rank if t[1]<=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Annotation').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LPT3966.bjss.co.uk:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Annotation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2915c32bfd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header','True').csv('CTRL_DBP_GEO_us_lakes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lake: string (nullable = true)\n",
      " |-- area: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_pyspark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-e08d6c608363>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_pyspark\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_pyspark' is not defined"
     ]
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
