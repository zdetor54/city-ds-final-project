{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--synthetic_column_size',\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help='Size of synthetic column')\n",
    "parser.add_argument(\n",
    "    '--sequence_size',\n",
    "    type=int,\n",
    "    default=50,\n",
    "    help='Length of word sequence of synthetic column')\n",
    "parser.add_argument(\n",
    "    '--model_dir',\n",
    "    type=str,\n",
    "    default=os.path.abspath('C:/Users/zacharias.detorakis/Desktop/city-ds-final-project/SemAIDA-master/AAAI19/exp_T2D/in_out/w2v_model/enwiki_model'),\n",
    "    # default='~/w2v_model/enwiki_model/',\n",
    "    help='Directory of word2vec model')\n",
    "FLAGS, unparsed = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cls_entities(file_name):\n",
    "    cls_entities = dict()\n",
    "    with open(os.path.join(os.getcwd(), file_name), 'r', encoding=\"utf-8\") as fun_f:\n",
    "        for line in fun_f.readlines():\n",
    "            line_tmp = line.strip().split('\",\"')\n",
    "            line_tmp[0] = line_tmp[0][1:]\n",
    "            line_tmp[-1] = line_tmp[-1][:-1]\n",
    "            cls_entities[line_tmp[0]] = line_tmp[1:]\n",
    "    return cls_entities\n",
    "\n",
    "def align_samples(pos, neg, pct = 0.5):\n",
    "    if len(pos) <= len(neg):\n",
    "        return pos+[random.choice(pos) for _ in range(math.ceil((len(neg)-len(pos))*pct))], neg\n",
    "    else:\n",
    "        return pos, neg+[random.choice(neg) for _ in range(math.ceil((len(pos)-len(neg))*pct))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = os.path.join(os.getcwd(), 'particular_neg_samples.csv')\n",
    "# df_target_col = pd.read_csv(file,header=None)\n",
    "cls_neg_par_entities = read_cls_entities('particular_neg_samples.csv')\n",
    "cls_pos_gen_entities = read_cls_entities('general_pos_samples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ents, n_ents = align_samples(cls_pos_gen_entities['GovernmentAgency'], cls_neg_par_entities['GovernmentAgency'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(pos): 1000, len(neg): 17126\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(pos): {len(p_ents)}, len(neg): {len(n_ents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_ents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n"
     ]
    }
   ],
   "source": [
    "from pattern.text.en import tokenize\n",
    "\n",
    "def generate_synthetic_columns(entities, synthetic_column_size):\n",
    "    ent_units = list()\n",
    "    if len(entities) >= synthetic_column_size:\n",
    "        for i, ent in enumerate(entities):\n",
    "            unit = random.sample(entities[0:i] + entities[(i + 1):], synthetic_column_size - 1)\n",
    "            unit.append(ent)\n",
    "            ent_units.append(unit)\n",
    "    else:\n",
    "        unit = entities + ['NaN'] * (len(entities) - synthetic_column_size)\n",
    "        ent_units.append(unit)\n",
    "    return ent_units\n",
    "\n",
    "def synthetic_columns2sequence(ent_units, sequence_size):\n",
    "    word_seq = list()\n",
    "    for ent in ent_units:\n",
    "        ent_n = ent.replace('_', ' ').replace('-', ' ').replace('.', ' ').replace('/', ' '). \\\n",
    "            replace('\"', ' ').replace(\"'\", ' ')\n",
    "        tokenized_line = ' '.join(tokenize(ent_n))\n",
    "        is_alpha_word_line = [word for word in tokenized_line.lower().split() if word.isalpha()]\n",
    "        word_seq += is_alpha_word_line\n",
    "    if len(word_seq) >= sequence_size:\n",
    "        return word_seq[0:sequence_size]\n",
    "    else:\n",
    "        return word_seq + ['NaN'] * (sequence_size - len(word_seq))\n",
    "    \n",
    "def sequence2matrix(word_seq, sequence_size, w2v_model):\n",
    "    ent_v = np.zeros((sequence_size, w2v_model.vector_size, 1))\n",
    "    for i, word in enumerate(word_seq):\n",
    "        if not word == 'NaN' and word in w2v_model.wv.vocab:\n",
    "            w_vec = w2v_model.wv[word]\n",
    "            ent_v[i] = w_vec.reshape((w2v_model.vector_size, 1))\n",
    "    return ent_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(entities_positive, entities_negative):\n",
    "    # embedding\n",
    "    units_positive = generate_synthetic_columns(entities_positive, FLAGS.synthetic_column_size)\n",
    "    units_negative = generate_synthetic_columns(entities_negative, FLAGS.synthetic_column_size)\n",
    "\n",
    "    sequences_positive = list()\n",
    "    for ent_unit in units_positive:\n",
    "        sequences_positive.append(synthetic_columns2sequence(ent_unit, FLAGS.sequence_size))\n",
    "    sequences_negative = list()\n",
    "    for ent_unit in units_negative:\n",
    "        sequences_negative.append(synthetic_columns2sequence(ent_unit, FLAGS.sequence_size))\n",
    "\n",
    "    x = np.zeros((len(sequences_positive) + len(sequences_negative), FLAGS.sequence_size, w2v_model.vector_size, 1))\n",
    "    for sample_i, sequence in enumerate(sequences_positive + sequences_negative):\n",
    "        x[sample_i] = sequence2matrix(sequence, FLAGS.sequence_size, w2v_model)\n",
    "\n",
    "    y_positive = np.zeros((len(sequences_positive), 2))\n",
    "    y_positive[:, 1] = 1.0\n",
    "    y_negative = np.zeros((len(sequences_negative), 2))\n",
    "    y_negative[:, 0] = 1.0\n",
    "    y = np.concatenate((y_positive, y_negative))\n",
    "\n",
    "    # shuffling\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(y.shape[0]))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "    return x_shuffled, y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(os.path.join(FLAGS.model_dir, 'word2vec_gensim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = embedding(p_ents, n_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10875\n"
     ]
    }
   ],
   "source": [
    "dev_sample_index = int(.6 * float(X.shape[0]))\n",
    "print(dev_sample_index)\n",
    "X_train, X_dev = X[dev_sample_index:], X[:dev_sample_index]\n",
    "Y_train, Y_dev = Y[dev_sample_index:], Y[:dev_sample_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://towardsdatascience.com/convolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "epochs = 5\n",
    "IMG_HEIGHT = X_train.shape[1]\n",
    "IMG_WIDTH = X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 50, 200, 16)       160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 25, 100, 16)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 25, 100, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 25, 100, 32)       4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 50, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 25, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 25, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               4915712   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 4,940,034\n",
      "Trainable params: 4,940,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Build the model\n",
    "model = Sequential([\n",
    "    Conv2D(16, 3, padding='same', activation='relu', \n",
    "           input_shape=(IMG_HEIGHT, IMG_WIDTH ,1)),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "# print the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 36s 318ms/step - loss: 0.5163 - accuracy: 0.9462\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, \n",
    "                       batch_size=64, \n",
    "                       epochs=1,  \n",
    "                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_dev[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dev[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7264"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "227 *32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7251, 50, 200, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('your_file.xls', 'w', encoding=\"utf-8\") as f:\n",
    "    for item in p_ents:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cls_pos_gen_entities['GovernmentAgency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-20cc02df08c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[1;34m'NaN'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "['NaN','2'] * 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  51 159 253\n",
      "  159  50   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  48 238 252 252\n",
      "  252 237   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  54 227 253 252 239\n",
      "  233 252  57   6   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202\n",
      "   84 252 253 122   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 163 252 252 252 253 252 252\n",
      "   96 189 253 167   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n",
      "   47  79 255 168   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  48 238 252 252 179  12  75 121  21\n",
      "    0   0 253 243  50   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0   0   0\n",
      "    0   0 253 252 165   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   7 178 252 240  71  19  28   0   0   0   0\n",
      "    0   0 253 252 195   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  57 252 252  63   0   0   0   0   0   0   0\n",
      "    0   0 253 252 195   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 198 253 190   0   0   0   0   0   0   0   0\n",
      "    0   0 255 253 196   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  76 246 252 112   0   0   0   0   0   0   0   0\n",
      "    0   0 253 252 148   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0\n",
      "    7 135 253 186  12   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 223   0   0   0   0   0   0   0   0   7\n",
      "  131 252 225  71   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n",
      "  252 173   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  86 253 225   0   0   0   0   0   0 114 238 253\n",
      "  162   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253 223 167\n",
      "   56   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  85 252 252 252 229 215 252 252 252 196 130   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  28 199 252 252 253 252 252 233 145   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  25 128 252 253 252 141  37   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise the set\n",
    "x_train = tf.keras.utils.normalize(x_train,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2649 - accuracy: 0.9237\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1100 - accuracy: 0.9665\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0733 - accuracy: 0.9769\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0556 - accuracy: 0.9818\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0418 - accuracy: 0.9862\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0317 - accuracy: 0.9899\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0256 - accuracy: 0.9915\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0208 - accuracy: 0.9930\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0187 - accuracy: 0.9937\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0160 - accuracy: 0.9944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2cc46040278>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten()) #input layer but also flatens the inputs from 28x28 to on vector\n",
    "\n",
    "# then add hidden layers\n",
    "model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam'\n",
    "             , loss = 'sparse_categorical_crossentropy'\n",
    "             , metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 94.1504 - accuracy: 0.9622\n",
      "94.15042877197266 0.9621999859809875\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(predictions[45]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[45])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
