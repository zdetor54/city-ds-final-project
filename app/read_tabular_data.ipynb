{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lookup candidate entities and classes\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "current_path = os.getcwd()\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--input_dir',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'data'),\n",
    "    help='Directory of input/output')\n",
    "parser.add_argument(\n",
    "    '--file_type',\n",
    "    type=str,\n",
    "    default='csv',\n",
    "    help='File type')\n",
    "\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "# if not os.path.exists(FLAGS.input_dir):\n",
    "#     os.mkdir(FLAGS.input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the csv files from the input directory\n",
    "def get_data_files(data_folder):\n",
    "    \"\"\"\n",
    "    A function used to get all the csv files from the input directory\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    data_folder : str\n",
    "        the folder within  the working directory where the data is located\n",
    "    \"\"\"\n",
    "\n",
    "    files = [] # a list of all filenames, including file extensions, that contain data\n",
    "    csv_files = [] # same list as above but without the file extension\n",
    "\n",
    "    # Get the list of files\n",
    "    files = [f for f in os.listdir(FLAGS.input_dir+data_folder) if os.path.isfile(os.path.join(FLAGS.input_dir+data_folder, f))]\n",
    "    csv_files = [f.replace(\".csv\",\"\") for f in os.listdir(FLAGS.input_dir+data_folder) if os.path.isfile(os.path.join(FLAGS.input_dir+data_folder, f))]\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "def get_target_cta_columns(target_config_file, csv_files, filter_col = True):\n",
    "    \"\"\"\n",
    "    A function used to get which columns from the csv files need to be considered for the CTA. This is a subset of the file columns ignoring anything that is not an entity\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    target_config_file : str\n",
    "        the file that contains the target column indices for each file\n",
    "    csv_files : list\n",
    "        the list of csv files that have the tabular data\n",
    "    filter_col : boolean\n",
    "        a flag to indicate whether we should narrow down the reading of the columns to only those targeted for the CTA task\n",
    "    \"\"\"\n",
    "   \n",
    "    target_col_file = os.path.join(FLAGS.input_dir, target_config_file)\n",
    "    df_target_col = pd.read_csv(target_col_file,header=None, names=['filename','column_index'])\n",
    "    \n",
    "    # filter to only those files that are included in the csv_files\n",
    "    df_target_col = df_target_col.loc[df_target_col['filename'].isin(csv_files)]\n",
    "    \n",
    "    # collapse all rows pertaining to the same file into one key value pair. The key is the filename and the value is the list with the column indices that should be considered\n",
    "    # dict_target = {'CTRL_DBP_GEO_european_countries_capital_populated_cities': [0, 1, 2]}\n",
    "    dict_target = dict()\n",
    "    \n",
    "    for index,row in df_target_col.iterrows():\n",
    "        \n",
    "        # is this is the first row with this file create the key\n",
    "        if row['filename'] not in dict_target:\n",
    "            dict_target[row['filename']]= []\n",
    "            \n",
    "        # append the new target column to the target column list for that file\n",
    "        if filter_col:\n",
    "            dict_target[row['filename']].append(row['column_index'])\n",
    "    \n",
    "    return dict_target\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '\\lite'\n",
    "# data_folder = '\\\\tables_full'\n",
    "\n",
    "# Get the list of csv files with tabular data\n",
    "csv_files = get_data_files(data_folder)\n",
    "# csv_files = get_data_files('\\\\tables_full')\n",
    "\n",
    "# Get the columns we need to consider for the CTA task\n",
    "dict_target_col = get_target_cta_columns('CTA_DBP_Round1_Targets.csv', csv_files,True)\n",
    "\n",
    "# dict_target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "data_folder = '\\lite'\n",
    "has_header_row = True\n",
    "\n",
    "for file in dict_target_col:\n",
    "    element = dict()\n",
    "    element['filename'] = file\n",
    "    df_data = pd.DataFrame()\n",
    "    df_title = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    \n",
    "    filename = file + '.' + FLAGS.file_type\n",
    "    tab_data_file = os.path.join(FLAGS.input_dir + data_folder, filename)\n",
    "      \n",
    "    # read the file data in a dataframe. Also read the column titles if we need to use them\n",
    "    if len(dict_target_col[file])>0:\n",
    "        if has_header_row:\n",
    "            df_data = pd.read_csv(tab_data_file,header=None, skiprows=[0], usecols=dict_target_col[file])\n",
    "            df_title = pd.read_csv(tab_data_file,header=None, usecols=dict_target_col[file], nrows = 1)\n",
    "        else:\n",
    "            df_data = pd.read_csv(tab_data_file,header=None, usecols=dict_target_col[file])\n",
    "    else:\n",
    "        if has_header_row:\n",
    "            df_data = pd.read_csv(tab_data_file,header=None, skiprows=[0])\n",
    "            df_title = pd.read_csv(tab_data_file,header=None, nrows = 1)\n",
    "        else:\n",
    "            df_data = pd.read_csv(tab_data_file,header=None)\n",
    "\n",
    "    # add the column headers to the data dictionary\n",
    "    try:\n",
    "        element['column_titles'] = list(df_title.iloc[0,:])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    file_element = dict()\n",
    "    for column in df_data.columns:\n",
    "        file_element[column] = list(set(df_data[column]))\n",
    "    element['data'] = file_element\n",
    "    \n",
    "    element['dataframe'] = df_data    \n",
    "    data.append(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_folder, dict_target_col, has_header_row = False):\n",
    "    data = list()\n",
    "\n",
    "    for file in dict_target_col:\n",
    "        element = dict()\n",
    "        element['filename'] = file\n",
    "        df_data = pd.DataFrame()\n",
    "        df_title = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "        filename = file + '.' + FLAGS.file_type\n",
    "        tab_data_file = os.path.join(FLAGS.input_dir + data_folder, filename)\n",
    "\n",
    "        # read the file data in a dataframe. Also read the column titles if we need to use them\n",
    "        if len(dict_target_col[file])>0:\n",
    "            if has_header_row:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None, skiprows=[0], usecols=dict_target_col[file])\n",
    "                df_title = pd.read_csv(tab_data_file,header=None, usecols=dict_target_col[file], nrows = 1)\n",
    "            else:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None, usecols=dict_target_col[file])\n",
    "        else:\n",
    "            if has_header_row:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None, skiprows=[0])\n",
    "                df_title = pd.read_csv(tab_data_file,header=None, nrows = 1)\n",
    "            else:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None)\n",
    "\n",
    "        # add the column headers to the data dictionary\n",
    "        try:\n",
    "            element['column_titles'] = list(df_title.iloc[0,:])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        file_element = dict()\n",
    "        for column in df_data.columns:\n",
    "            file_element[column] = list(set(df_data[column]))\n",
    "        element['data'] = file_element\n",
    "\n",
    "        element['dataframe'] = df_data    \n",
    "        data.append(element)\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def retrieve_dbpedia_classes (query_string, entity_classes, max_hits = 5):\n",
    "    web_api = 'http://lookup.dbpedia.org/api/search/KeywordSearch?MaxHits=%s&QueryString=%s'\n",
    "    dbo_prefix = 'http://dbpedia.org/ontology/'\n",
    "    dbp_prefix = 'http://dbpedia.org/resource/'\n",
    "    \n",
    "#     entity_classes = dict()\n",
    "    try:\n",
    "        lookup_url = web_api % (max_hits, query_string)\n",
    "#         print(lookup_url)\n",
    "        lookup_res = requests.get(lookup_url)\n",
    "        root = ET.fromstring(lookup_res.content)\n",
    "        i=0\n",
    "        for child in root:\n",
    "            i+=1\n",
    "#             print(\"\\n\\n\\n\")\n",
    "            print(child[1].text)\n",
    "            entity = child[1].text.split(dbp_prefix)[1]\n",
    "#             print(entity)\n",
    "            classes = list()\n",
    "            for cc in child[3]:\n",
    "                cls_URI = cc[1].text\n",
    "#                 print(cls_URI)\n",
    "                if dbo_prefix in cls_URI:\n",
    "                    classes.append((cls_URI.split(dbo_prefix)[1],i))\n",
    "            if len(classes)>0:\n",
    "                entity_classes[entity] = classes\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "    return entity_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(data_folder, dict_target_col, True)\n",
    "# data[0]['data'][2]\n",
    "# data[1]['column_titles']\n",
    "# data[0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dbpedia.org/resource/Budapest\n",
      "http://dbpedia.org/resource/Hungary\n",
      "http://dbpedia.org/resource/Cluj-Napoca\n",
      "http://dbpedia.org/resource/Debrecen\n",
      "http://dbpedia.org/resource/Miskolc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Budapest': [('Settlement', 1),\n",
       "  ('City', 1),\n",
       "  ('Place', 1),\n",
       "  ('PopulatedPlace', 1),\n",
       "  ('Location', 1)],\n",
       " 'Hungary': [('Place', 2),\n",
       "  ('Country', 2),\n",
       "  ('PopulatedPlace', 2),\n",
       "  ('Location', 2)],\n",
       " 'Cluj-Napoca': [('Settlement', 3),\n",
       "  ('City', 3),\n",
       "  ('Place', 3),\n",
       "  ('PopulatedPlace', 3),\n",
       "  ('Location', 3)],\n",
       " 'Debrecen': [('Settlement', 4),\n",
       "  ('City', 4),\n",
       "  ('Place', 4),\n",
       "  ('PopulatedPlace', 4),\n",
       "  ('Location', 4)],\n",
       " 'Miskolc': [('Settlement', 5),\n",
       "  ('City', 5),\n",
       "  ('Place', 5),\n",
       "  ('PopulatedPlace', 5),\n",
       "  ('Location', 5)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_classes = dict()\n",
    "entity_classes = retrieve_dbpedia_classes('Capital City of Budapest',entity_classes,5)\n",
    "entity_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_classes = dict()\n",
    "for entity in data[2]['data'][4]:\n",
    "    entity_classes = retrieve_dbpedia_classes(entity,entity_classes,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wien',\n",
       " 'Thessaloniki',\n",
       " 'Batumi',\n",
       " 'Limassol',\n",
       " 'Ganja',\n",
       " 'Capital City of Budapest',\n",
       " 'Kharkiv (Харків)',\n",
       " 'Luzern',\n",
       " 'Korzeniew',\n",
       " 'Almaty',\n",
       " 'Kaunas',\n",
       " 'Haraçinë',\n",
       " 'Poitiers',\n",
       " 'Tallinn',\n",
       " 'Ljubljana',\n",
       " 'Երևան',\n",
       " 'Pristina',\n",
       " 'Tuzla',\n",
       " 'Bergen',\n",
       " 'Budapest főváros',\n",
       " 'Istanbul',\n",
       " 'Cwmffrwd',\n",
       " 'Yerevan',\n",
       " 'Budapest',\n",
       " 'Nendeln',\n",
       " 'Carei',\n",
       " 'Sofia',\n",
       " 'Minsk',\n",
       " 'Arinsal',\n",
       " 'Palić',\n",
       " 'Vienna',\n",
       " 'City of Podgorica',\n",
       " 'Ħal Qormi',\n",
       " 'Göteborg',\n",
       " 'İstanbul',\n",
       " 'Палић',\n",
       " 'Qormi',\n",
       " 'Zagreb',\n",
       " 'Rīga',\n",
       " 'Riga',\n",
       " 'Aračinovo / Arachinovo',\n",
       " 'Chișinău',\n",
       " 'Prague',\n",
       " 'Madrid',\n",
       " 'Malé Dvorníky',\n",
       " 'Λεμεσός / Limasol',\n",
       " 'City of Pristina',\n",
       " 'Арачиново',\n",
       " 'Dogana',\n",
       " 'Odense',\n",
       " 'Алматы',\n",
       " 'Kharkov (Харьков)',\n",
       " 'Praha',\n",
       " 'МінскМинск',\n",
       " 'София',\n",
       " 'Metropolitan City of Rome Capital',\n",
       " 'Podgorica',\n",
       " 'Тузла',\n",
       " 'Gothenburg',\n",
       " 'Gəncə',\n",
       " 'Hlavní město Praha',\n",
       " 'City of Zagreb',\n",
       " 'Latin:Ursaria']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['data'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
