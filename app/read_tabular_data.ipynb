{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lookup candidate entities and classes\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "current_path = os.getcwd()\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--input_dir',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'data'),\n",
    "    help='Directory of input/output')\n",
    "parser.add_argument(\n",
    "    '--file_type',\n",
    "    type=str,\n",
    "    default='csv',\n",
    "    help='File type')\n",
    "\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "# if not os.path.exists(FLAGS.input_dir):\n",
    "#     os.mkdir(FLAGS.input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the csv files from the input directory\n",
    "def get_data_files(data_folder):\n",
    "    \"\"\"\n",
    "    A function used to get all the csv files from the input directory\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    data_folder : str\n",
    "        the folder within  the working directory where the data is located\n",
    "    \"\"\"\n",
    "\n",
    "    files = [] # a list of all filenames, including file extensions, that contain data\n",
    "    csv_files = [] # same list as above but without the file extension\n",
    "\n",
    "    # Get the list of files\n",
    "    files = [f for f in os.listdir(FLAGS.input_dir+data_folder) if os.path.isfile(os.path.join(FLAGS.input_dir+data_folder, f))]\n",
    "    csv_files = [f.replace(\".csv\",\"\") for f in os.listdir(FLAGS.input_dir+data_folder) if os.path.isfile(os.path.join(FLAGS.input_dir+data_folder, f))]\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "def get_target_cta_columns(target_config_file, csv_files, filter_col = True):\n",
    "    \"\"\"\n",
    "    A function used to get which columns from the csv files need to be considered for the CTA. This is a subset of the file columns ignoring anything that is not an entity\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    target_config_file : str\n",
    "        the file that contains the target column indices for each file\n",
    "    csv_files : list\n",
    "        the list of csv files that have the tabular data\n",
    "    filter_col : boolean\n",
    "        a flag to indicate whether we should narrow down the reading of the columns to only those targeted for the CTA task\n",
    "    \"\"\"\n",
    "   \n",
    "    target_col_file = os.path.join(FLAGS.input_dir, target_config_file)\n",
    "    df_target_col = pd.read_csv(target_col_file,header=None, names=['filename','column_index'])\n",
    "    \n",
    "    # filter to only those files that are included in the csv_files\n",
    "    df_target_col = df_target_col.loc[df_target_col['filename'].isin(csv_files)]\n",
    "    \n",
    "    # collapse all rows pertaining to the same file into one key value pair. The key is the filename and the value is the list with the column indices that should be considered\n",
    "    # dict_target = {'CTRL_DBP_GEO_european_countries_capital_populated_cities': [0, 1, 2]}\n",
    "    dict_target = dict()\n",
    "    \n",
    "    for index,row in df_target_col.iterrows():\n",
    "        \n",
    "        # is this is the first row with this file create the key\n",
    "        if row['filename'] not in dict_target:\n",
    "            dict_target[row['filename']]= []\n",
    "            \n",
    "        # append the new target column to the target column list for that file\n",
    "        if filter_col:\n",
    "            dict_target[row['filename']].append(row['column_index'])\n",
    "    \n",
    "    return dict_target\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '\\lite'\n",
    "# data_folder = '\\\\tables_full'\n",
    "\n",
    "# Get the list of csv files with tabular data\n",
    "csv_files = get_data_files(data_folder)\n",
    "# csv_files = get_data_files('\\\\tables_full')\n",
    "\n",
    "# Get the columns we need to consider for the CTA task\n",
    "dict_target_col = get_target_cta_columns('CTA_DBP_Round1_Targets.csv', csv_files,True)\n",
    "\n",
    "# dict_target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "data_folder = '\\lite'\n",
    "has_header_row = True\n",
    "\n",
    "for file in dict_target_col:\n",
    "    element = dict()\n",
    "    element['filename'] = file\n",
    "    df_data = pd.DataFrame()\n",
    "    df_title = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    \n",
    "    filename = file + '.' + FLAGS.file_type\n",
    "    tab_data_file = os.path.join(FLAGS.input_dir + data_folder, filename)\n",
    "      \n",
    "    # read the file data in a dataframe. Also read the column titles if we need to use them\n",
    "    if len(dict_target_col[file])>0:\n",
    "        if has_header_row:\n",
    "            df_data = pd.read_csv(tab_data_file,header=None, skiprows=[0], usecols=dict_target_col[file])\n",
    "            df_title = pd.read_csv(tab_data_file,header=None, usecols=dict_target_col[file], nrows = 1)\n",
    "        else:\n",
    "            df_data = pd.read_csv(tab_data_file,header=None, usecols=dict_target_col[file])\n",
    "    else:\n",
    "        if has_header_row:\n",
    "            df_data = pd.read_csv(tab_data_file,header=None, skiprows=[0])\n",
    "            df_title = pd.read_csv(tab_data_file,header=None, nrows = 1)\n",
    "        else:\n",
    "            df_data = pd.read_csv(tab_data_file,header=None)\n",
    "\n",
    "    # add the column headers to the data dictionary\n",
    "    try:\n",
    "        element['column_titles'] = list(df_title.iloc[0,:])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    file_element = dict()\n",
    "    for column in df_data.columns:\n",
    "        file_element[column] = list(set(df_data[column]))\n",
    "    element['data'] = file_element\n",
    "    \n",
    "    element['dataframe'] = df_data    \n",
    "    data.append(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_folder, dict_target_col, has_header_row = False):\n",
    "    data = list()\n",
    "\n",
    "    for file in dict_target_col:\n",
    "        element = dict()\n",
    "        element['filename'] = file\n",
    "        df_data = pd.DataFrame()\n",
    "        df_title = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "        filename = file + '.' + FLAGS.file_type\n",
    "        tab_data_file = os.path.join(FLAGS.input_dir + data_folder, filename)\n",
    "\n",
    "        # read the file data in a dataframe. Also read the column titles if we need to use them\n",
    "        if len(dict_target_col[file])>0:\n",
    "            if has_header_row:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None, skiprows=[0], usecols=dict_target_col[file])\n",
    "                df_title = pd.read_csv(tab_data_file,header=None, usecols=dict_target_col[file], nrows = 1)\n",
    "            else:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None, usecols=dict_target_col[file])\n",
    "        else:\n",
    "            if has_header_row:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None, skiprows=[0])\n",
    "                df_title = pd.read_csv(tab_data_file,header=None, nrows = 1)\n",
    "            else:\n",
    "                df_data = pd.read_csv(tab_data_file,header=None)\n",
    "\n",
    "        # add the column headers to the data dictionary\n",
    "        try:\n",
    "            element['column_titles'] = list(df_title.iloc[0,:])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        file_element = dict()\n",
    "        for column in df_data.columns:\n",
    "            file_element[column] = list(set(df_data[column]))\n",
    "        element['data'] = file_element\n",
    "\n",
    "        element['dataframe'] = df_data    \n",
    "        data.append(element)\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def retrieve_dbpedia_classes (query_string, entity_classes, max_hits = 5):\n",
    "    web_api = 'http://lookup.dbpedia.org/api/search/KeywordSearch?MaxHits=%s&QueryString=%s'\n",
    "    dbo_prefix = 'http://dbpedia.org/ontology/'\n",
    "    dbp_prefix = 'http://dbpedia.org/resource/'\n",
    "    \n",
    "#     entity_classes = dict()\n",
    "    try:\n",
    "        lookup_url = web_api % (max_hits, query_string)\n",
    "#         print(lookup_url)\n",
    "        lookup_res = requests.get(lookup_url)\n",
    "        root = ET.fromstring(lookup_res.content)\n",
    "        i=0\n",
    "        for child in root:\n",
    "            i+=1\n",
    "#             print(\"\\n\\n\\n\")\n",
    "#             print(child[1].text)\n",
    "            entity = child[1].text.split(dbp_prefix)[1]\n",
    "#             print(entity)\n",
    "            classes = list()\n",
    "            for cc in child[3]:\n",
    "                cls_URI = cc[1].text\n",
    "#                 print(cls_URI)\n",
    "                if dbo_prefix in cls_URI:\n",
    "                    classes.append((cls_URI.split(dbo_prefix)[1],i))\n",
    "            if len(classes)>0:\n",
    "                entity_classes[entity] = classes\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "    return entity_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(data_folder, dict_target_col, True)\n",
    "# data[0]['data'][2]\n",
    "# data[1]['column_titles']\n",
    "# data[0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Budapest': [('Settlement', 1),\n",
       "  ('City', 1),\n",
       "  ('Place', 1),\n",
       "  ('PopulatedPlace', 1),\n",
       "  ('Location', 1)],\n",
       " 'Hungary': [('Place', 2),\n",
       "  ('Country', 2),\n",
       "  ('PopulatedPlace', 2),\n",
       "  ('Location', 2)],\n",
       " 'Cluj-Napoca': [('Settlement', 3),\n",
       "  ('City', 3),\n",
       "  ('Place', 3),\n",
       "  ('PopulatedPlace', 3),\n",
       "  ('Location', 3)],\n",
       " 'Debrecen': [('Settlement', 4),\n",
       "  ('City', 4),\n",
       "  ('Place', 4),\n",
       "  ('PopulatedPlace', 4),\n",
       "  ('Location', 4)],\n",
       " 'Miskolc': [('Settlement', 5),\n",
       "  ('City', 5),\n",
       "  ('Place', 5),\n",
       "  ('PopulatedPlace', 5),\n",
       "  ('Location', 5)]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_classes = dict()\n",
    "entity_classes = retrieve_dbpedia_classes('Capital City of Budapest',entity_classes,5)\n",
    "entity_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.57\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "entity_classes = dict()\n",
    "i=0;\n",
    "for entity in data[2]['data'][4]:\n",
    "    entity_classes = retrieve_dbpedia_classes(entity,entity_classes,2)\n",
    "    clear_output(wait=True)\n",
    "    print(round(i*100 / len(data[2]['data'][4]),2))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AdministrativeRegion',\n",
       " 'Place',\n",
       " 'PopulatedPlace',\n",
       " 'Location',\n",
       " 'Region',\n",
       " 'Settlement',\n",
       " 'City',\n",
       " 'Organisation',\n",
       " 'SportsTeam',\n",
       " 'AmericanFootballTeam',\n",
       " 'Agent',\n",
       " 'University',\n",
       " 'EducationalInstitution',\n",
       " 'SoccerClub',\n",
       " 'SportsClub',\n",
       " 'Criminal',\n",
       " 'Building',\n",
       " 'ArchitecturalStructure',\n",
       " 'Town',\n",
       " 'BodyOfWater',\n",
       " 'River',\n",
       " 'Stream',\n",
       " 'Politician',\n",
       " 'Person',\n",
       " 'SoccerLeague',\n",
       " 'SportsLeague',\n",
       " 'Lake',\n",
       " 'Road',\n",
       " 'Infrastructure',\n",
       " 'RouteOfTransportation',\n",
       " 'EthnicGroup',\n",
       " 'CityDistrict',\n",
       " 'BasketballLeague',\n",
       " 'ProtectedArea',\n",
       " 'School',\n",
       " 'SupremeCourtOfTheUnitedStatesCase',\n",
       " 'UnitOfWork',\n",
       " 'Case',\n",
       " 'LegalCase',\n",
       " 'Monument',\n",
       " 'HorseRace',\n",
       " 'Event',\n",
       " 'SportsEvent',\n",
       " 'SocietalEvent',\n",
       " 'Race',\n",
       " 'TelevisionShow',\n",
       " 'Work',\n",
       " 'Mountain',\n",
       " 'NaturalPlace',\n",
       " 'BaseballTeam',\n",
       " 'HockeyTeam',\n",
       " 'Village',\n",
       " 'HistoricPlace',\n",
       " 'Dam',\n",
       " 'TelevisionStation',\n",
       " 'Broadcaster',\n",
       " 'FormerMunicipality',\n",
       " 'GovernmentalAdministrativeRegion',\n",
       " 'Municipality',\n",
       " 'Airport']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_classes = list([])\n",
    "candidate_classes_rank = list([])\n",
    "for key in entity_classes:\n",
    "    for candicate_class in entity_classes[key]:\n",
    "        cc,rank = candicate_class\n",
    "        if cc not in candidate_classes:\n",
    "            candidate_classes.append(cc)\n",
    "            candidate_classes_rank.append((cc,rank))\n",
    "\n",
    "candidate_classes_rank = sorted(candidate_classes_rank, key=lambda x: x[1])\n",
    "[t[0] for t in candidate_classes_rank if t[1]<=100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "C:\\Users\\zacharias.detorakis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 60)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "model.train([[t[0] for t in candidate_classes_rank if t[1]<=100]], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
